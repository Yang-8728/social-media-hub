from flask import Flask, jsonify, request
import redis
import json
import os
import threading
import time
import sys
import subprocess
import glob

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append('/app')

app = Flask(__name__)
redis_client = redis.from_url('redis://redis:6379')

# åŠ è½½è´¦å·é…ç½®
def load_account_config():
    config_path = '/app/config/accounts.json'
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

@app.route('/')
def home():
    return jsonify({'service': 'merger', 'status': 'running'})

@app.route('/merge', methods=['POST'])
def start_merge():
    """å¯åŠ¨è§†é¢‘åˆå¹¶ä»»åŠ¡"""
    data = request.get_json()
    account_name = data.get('account')
    limit = data.get('limit', None)
    video_folder = data.get('video_folder', None)  # æ–°å¢ï¼šæŒ‡å®šè§†é¢‘æ–‡ä»¶å¤¹
    
    if not account_name:
        return jsonify({'error': 'account is required'}), 400
    
    # åŠ è½½è´¦å·é…ç½®
    accounts_config = load_account_config()
    account_config = accounts_config.get(account_name, {})
    
    if not account_config:
        return jsonify({'error': f'Account {account_name} not found in config'}), 404
    
    # æ„å»ºåˆå¹¶ä»»åŠ¡
    task = {
        'account': account_name,
        'limit': limit,
        'video_folder': video_folder,  # æ–°å¢å­—æ®µ
        'type': 'merge',
        'status': 'pending'
    }
    
    redis_client.lpush('merge_queue', json.dumps(task))
    
    return jsonify({
        'message': f'Merge task started',
        'account': account_name,
        'limit': limit,
        'video_folder': video_folder
    })

def process_merge_queue():
    """å¤„ç†åˆå¹¶é˜Ÿåˆ—çš„å·¥ä½œè¿›ç¨‹"""
    while True:
        try:
            # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
            task_data = redis_client.brpop('merge_queue', timeout=5)
            if not task_data:
                continue
                
            task = json.loads(task_data[1])
            
            print(f"ğŸ”„ å¤„ç†åˆå¹¶ä»»åŠ¡: {task}", flush=True)
            
            if task['type'] == 'merge':
                process_merge_task(task)
                
        except Exception as e:
            print(f"âŒ å¤„ç†é˜Ÿåˆ—ä»»åŠ¡å‡ºé”™: {e}", flush=True)
            time.sleep(1)

def process_merge_task(task):
    """å¤„ç†è§†é¢‘åˆå¹¶ä»»åŠ¡ - å¾®æœåŠ¡æ¶æ„ç‰ˆæœ¬ï¼ˆè°ƒç”¨standardizerï¼‰"""
    try:
        account_name = task['account']
        limit = task.get('limit')
        video_folder = task.get('video_folder', None)  # æ–°å¢ï¼šæŒ‡å®šæ–‡ä»¶å¤¹
        date = task.get('date', None)  # æ–°å¢ï¼šæŒ‡å®šæ—¥æœŸ
        is_pipeline = task.get('pipeline', False)
        
        print(f"ğŸ¬ å¼€å§‹åˆå¹¶ä»»åŠ¡: {account_name}, é™åˆ¶: {limit}, æ—¥æœŸ: {date}, æ–‡ä»¶å¤¹: {video_folder}", flush=True)
        
        # ä½¿ç”¨å¾®æœåŠ¡æ¶æ„
        result = process_merge_with_microservices(account_name, limit, video_folder, date)
        
        print(f"âœ… åˆå¹¶å®Œæˆ: {result}", flush=True)
        
        # å°†ç»“æœå­˜å‚¨åˆ°Redisä»¥ä¾›æŸ¥è¯¢
        redis_client.setex(f"merge_result_{account_name}", 3600, json.dumps(result))
        
        # å¦‚æœæ˜¯æµæ°´çº¿ä»»åŠ¡ä¸”åˆå¹¶æˆåŠŸï¼Œè‡ªåŠ¨è§¦å‘ä¸Šä¼ 
        if is_pipeline and result.get('merged', 0) > 0:
            print(f"ğŸš€ æµæ°´çº¿æ¨¡å¼ï¼šè‡ªåŠ¨è§¦å‘ä¸Šä¼ ä»»åŠ¡", flush=True)
            upload_task = {
                'account': account_name,
                'video_path': result.get('output_file'),
                'title': None,  # ä»æ–‡ä»¶åæå–
                'status': 'pending'
            }
            redis_client.lpush('upload_queue', json.dumps(upload_task))
            print(f"ğŸ“¤ å·²æ·»åŠ ä¸Šä¼ ä»»åŠ¡åˆ°é˜Ÿåˆ—", flush=True)
        
    except Exception as e:
        print(f"âŒ åˆå¹¶ä»»åŠ¡å¤±è´¥: {e}", flush=True)
        import traceback
        traceback.print_exc()
        error_result = {"merged": 0, "skipped": 0, "failed": 1, "error": str(e)}
        redis_client.setex(f"merge_result_{account_name}", 3600, json.dumps(error_result))

def process_merge_with_microservices(account_name: str, limit: int = None, video_folder: str = None, date: str = None):
    """ä½¿ç”¨å¾®æœåŠ¡æ¶æ„è¿›è¡Œåˆå¹¶: Standardizer + Mergeråˆ†ç¦»
    
    Args:
        account_name: è´¦å·åç§°
        limit: é™åˆ¶åˆå¹¶æ•°é‡
        video_folder: æŒ‡å®šè§†é¢‘æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        date: æŒ‡å®šæ—¥æœŸ YYYY-MM-DDï¼ˆå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨ä»Šå¤©ï¼‰
    """
    try:
        # å¦‚æœæŒ‡å®šäº† video_folderï¼Œç›´æ¥ä½¿ç”¨è¯¥æ–‡ä»¶å¤¹çš„è§†é¢‘
        if video_folder:
            print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šæ–‡ä»¶å¤¹: {video_folder}", flush=True)
            
            if not os.path.exists(video_folder):
                print(f"âš ï¸ æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {video_folder}", flush=True)
                return {"merged": 0, "skipped": 0, "failed": 0}
            
            # è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è§†é¢‘
            all_videos = []
            for ext in ['mp4', 'avi', 'mov']:
                all_videos.extend(glob.glob(os.path.join(video_folder, f"*.{ext}")))
            
            all_videos = sorted(all_videos, reverse=True)  # æœ€æ–°çš„åœ¨å‰
            
            if not all_videos:
                print(f"âš ï¸ æŒ‡å®šæ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶", flush=True)
                return {"merged": 0, "skipped": 0, "failed": 0}
            
            print(f"ğŸ“¹ æ‰¾åˆ° {len(all_videos)} ä¸ªè§†é¢‘æ–‡ä»¶", flush=True)
            
            # åº”ç”¨limit
            if limit:
                videos_to_merge = all_videos[:limit]
                print(f"ğŸ“¹ å‡†å¤‡åˆå¹¶æœ€æ–°çš„ {len(videos_to_merge)} ä¸ªè§†é¢‘ï¼ˆå‰©ä½™ {len(all_videos) - len(videos_to_merge)} ä¸ªï¼‰", flush=True)
            else:
                videos_to_merge = all_videos
                print(f"ğŸ“¹ å‡†å¤‡åˆå¹¶å…¨éƒ¨ {len(videos_to_merge)} ä¸ªè§†é¢‘", flush=True)
            
            print(f"   è§†é¢‘åˆ—è¡¨: {[os.path.basename(v) for v in videos_to_merge]}", flush=True)
            
            # ç›´æ¥åˆå¹¶ï¼ˆæ–‡ä»¶å·²ç»æ ‡å‡†åŒ–ï¼‰
            return merge_standardized_videos(account_name, videos_to_merge, video_folder)
        
        # åŸæœ‰é€»è¾‘ï¼šå¤„ç†æŒ‡å®šæ—¥æœŸæˆ–ä»Šå¤©çš„è§†é¢‘
        print(f"ğŸ“‹ æ­¥éª¤1: æ‰«æè§†é¢‘æ–‡ä»¶", flush=True)
        
        # è·å–æ—¥æœŸï¼ˆä½¿ç”¨æŒ‡å®šæ—¥æœŸæˆ–ä»Šå¤©ï¼‰
        from datetime import datetime
        if date:
            target_date = date
            print(f"   ğŸ—“ï¸  ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: {target_date}", flush=True)
        else:
            target_date = datetime.now().strftime("%Y-%m-%d")
            print(f"   ğŸ—“ï¸  ä½¿ç”¨ä»Šå¤©æ—¥æœŸ: {target_date}", flush=True)
        
        # æ‰«æä¸‹è½½ç›®å½•
        downloads_base = f"/app/videos/downloads/{account_name}"
        
        if not os.path.exists(downloads_base):
            print(f"âš ï¸ ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {downloads_base}", flush=True)
            return {"merged": 0, "skipped": 0, "failed": 0}
        
        # è·å–è´¦æˆ·é…ç½®çš„folder_strategy
        try:
            from main import load_account_config
            account_configs = load_account_config()
            account_config = account_configs.get(account_name, {})
            folder_strategy = account_config.get("folder_strategy", "daily")
        except:
            folder_strategy = "daily"
        
        # æ ¹æ®ç­–ç•¥æŸ¥æ‰¾æŒ‡å®šæ—¥æœŸçš„è§†é¢‘
        all_today_videos = []
        
        if folder_strategy == "date_blogger":
            # date_bloggeræ ¼å¼ï¼šYYYY-MM-DD_åšä¸»ID
            pattern = os.path.join(downloads_base, f"{target_date}_*")
            today_folders = glob.glob(pattern)
            
            for folder in today_folders:
                if os.path.isdir(folder):
                    videos = glob.glob(os.path.join(folder, "*.mp4"))
                    all_today_videos.extend(videos)
        else:
            # dailyæ ¼å¼ï¼šYYYY-MM-DD
            today_path = os.path.join(downloads_base, target_date)
            if os.path.exists(today_path):
                videos = glob.glob(os.path.join(today_path, "*.mp4"))
                all_today_videos.extend(videos)
        
        if not all_today_videos:
            print(f"â„¹ï¸  æ²¡æœ‰æ‰¾åˆ° {target_date} çš„è§†é¢‘æ–‡ä»¶", flush=True)
            return {"merged": 0, "skipped": 0, "failed": 0}
        
        # æ£€æŸ¥å“ªäº›å·²ç»åˆå¹¶è¿‡
        from src.utils.video_merger import VideoMerger
        temp_merger = VideoMerger(account_name)
        
        unmerged_videos = []
        skipped_count = 0
        for video in all_today_videos:
            if temp_merger.is_video_merged(video):
                skipped_count += 1
            else:
                unmerged_videos.append(video)
        
        if skipped_count > 0:
            print(f"ğŸ“Š {target_date} æ‰¾åˆ° {len(all_today_videos)} ä¸ªè§†é¢‘ï¼Œå…¶ä¸­ {skipped_count} ä¸ªå·²åˆå¹¶ï¼Œ{len(unmerged_videos)} ä¸ªå¾…åˆå¹¶", flush=True)
        
        if not unmerged_videos:
            print(f"â„¹ï¸  {target_date} æ‰€æœ‰è§†é¢‘éƒ½å·²ç»åˆå¹¶è¿‡äº†", flush=True)
            return {"merged": 0, "skipped": skipped_count, "failed": 0}
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        unmerged_videos.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        # åº”ç”¨é™åˆ¶
        if limit:
            merge_videos = unmerged_videos[:limit]
            print(f"ğŸ“¹ å‡†å¤‡åˆå¹¶æœ€æ–°çš„ {len(merge_videos)} ä¸ªè§†é¢‘ï¼ˆå‰©ä½™ {len(unmerged_videos) - len(merge_videos)} ä¸ªï¼‰", flush=True)
        else:
            merge_videos = unmerged_videos
            print(f"ğŸ“¹ å‡†å¤‡åˆå¹¶å…¨éƒ¨ {len(unmerged_videos)} ä¸ªæœªåˆå¹¶è§†é¢‘", flush=True)
        
        print(f"   è§†é¢‘åˆ—è¡¨: {[os.path.basename(v) for v in merge_videos]}", flush=True)
        
        # æ­¥éª¤2: è°ƒç”¨StandardizeræœåŠ¡è¿›è¡Œæ ‡å‡†åŒ–
        print(f"ğŸ¨ æ­¥éª¤2: è°ƒç”¨StandardizeræœåŠ¡è¿›è¡Œæ ‡å‡†åŒ–...", flush=True)
        
        import requests
        standardize_task = {
            'account': account_name,
            'video_files': merge_videos,
            'output_folder': f'/app/videos/standardized/{account_name}',
            'process_type': 'ultimate'
        }
        
        standardize_response = requests.post(
            'http://standardizer:8000/process-batch',
            json=standardize_task,
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
        )
        
        if standardize_response.status_code != 200:
            raise Exception(f"StandardizeræœåŠ¡å¤±è´¥: {standardize_response.text}")
        
        standardize_result = standardize_response.json()
        print(f"âœ… è§†é¢‘æ ‡å‡†åŒ–ä»»åŠ¡å·²åŠ å…¥é˜Ÿåˆ—: {standardize_result}", flush=True)
        
        # æ­¥éª¤3: ç­‰å¾…æ ‡å‡†åŒ–å®Œæˆ
        print(f"â³ æ­¥éª¤3: ç­‰å¾…æ ‡å‡†åŒ–å®Œæˆ...", flush=True)
        
        standardized_folder = f"/app/videos/standardized/{account_name}"
        os.makedirs(standardized_folder, exist_ok=True)
        
        # ç­‰å¾…æ ‡å‡†åŒ–æ–‡ä»¶å‡ºç°ï¼ˆæœ€å¤š10åˆ†é’Ÿï¼‰
        wait_timeout = 600  # 10åˆ†é’Ÿ
        wait_interval = 5  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
        waited_time = 0
        
        while waited_time < wait_timeout:
            standardized_files = glob.glob(os.path.join(standardized_folder, "*_ultimate.mp4"))
            if len(standardized_files) >= len(merge_videos):
                print(f"âœ… æ ‡å‡†åŒ–å®Œæˆï¼æ‰¾åˆ° {len(standardized_files)} ä¸ªæ–‡ä»¶", flush=True)
                break
            
            print(f"   ç­‰å¾…ä¸­... ({waited_time}s/{wait_timeout}s), å½“å‰æ–‡ä»¶æ•°: {len(standardized_files)}/{len(merge_videos)}", flush=True)
            time.sleep(wait_interval)
            waited_time += wait_interval
        else:
            raise Exception(f"ç­‰å¾…æ ‡å‡†åŒ–è¶…æ—¶ ({wait_timeout}s)ï¼Œåªæ‰¾åˆ° {len(standardized_files)} ä¸ªæ–‡ä»¶")
        
        # æ­¥éª¤4: åˆå¹¶æ ‡å‡†åŒ–åçš„è§†é¢‘
        print(f"ğŸ”— æ­¥éª¤4: åˆå¹¶æ ‡å‡†åŒ–åçš„è§†é¢‘...", flush=True)
        
        standardized_folder = f"/app/videos/standardized/{account_name}"
        output_folder = f"/app/videos/merged/{account_name}"
        os.makedirs(output_folder, exist_ok=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_filename = temp_merger._generate_title_filename(merge_videos)
        output_path = os.path.join(output_folder, output_filename)
        
        # åˆ›å»ºconcatæ–‡ä»¶åˆ—è¡¨
        concat_file = f"/tmp/concat_{account_name}.txt"
        standardized_files = glob.glob(os.path.join(standardized_folder, "*.mp4"))
        standardized_files.sort()  # ç¡®ä¿é¡ºåº
        
        print(f"   æ‰¾åˆ° {len(standardized_files)} ä¸ªæ ‡å‡†åŒ–æ–‡ä»¶", flush=True)
        
        with open(concat_file, 'w', encoding='utf-8') as f:
            for std_file in standardized_files:
                f.write(f"file '{std_file}'\n")
        
        # FFmpegåˆå¹¶å‘½ä»¤
        merge_cmd = [
            'ffmpeg',
            '-f', 'concat',
            '-safe', '0',
            '-i', concat_file,
            '-c', 'copy',
            '-y',
            output_path
        ]
        
        result_code = subprocess.run(merge_cmd, capture_output=True, text=True, encoding='utf-8', errors='replace')
        
        if result_code.returncode == 0 and os.path.exists(output_path):
            output_size_mb = os.path.getsize(output_path) / (1024*1024)
            print(f"âœ… åˆå¹¶æˆåŠŸ! è¾“å‡ºæ–‡ä»¶: {output_path} ({output_size_mb:.1f}MB)", flush=True)
            
            # è®°å½•å·²åˆå¹¶è§†é¢‘
            temp_merger.add_merged_videos(merge_videos, output_path)
            
            return {"merged": 1, "skipped": skipped_count, "failed": 0, "output_file": output_path}
        else:
            print(f"âŒ FFmpegåˆå¹¶å¤±è´¥: {result_code.stderr}", flush=True)
            return {"merged": 0, "skipped": skipped_count, "failed": 1}
            
    except Exception as e:
        print(f"âŒ å¾®æœåŠ¡åˆå¹¶æµç¨‹å¤±è´¥: {e}", flush=True)
        import traceback
        traceback.print_exc()
        return {"merged": 0, "skipped": 0, "failed": 1, "error": str(e)}

@app.route('/status/<account_name>')
def get_status(account_name):
    """è·å–åˆå¹¶çŠ¶æ€"""
    result = redis_client.get(f"merge_result_{account_name}")
    if result:
        return jsonify(json.loads(result))
    else:
        return jsonify({'status': 'no recent tasks'})

def merge_standardized_videos(account_name: str, video_files: list, source_folder: str):
    """ç›´æ¥åˆå¹¶å·²æ ‡å‡†åŒ–çš„è§†é¢‘æ–‡ä»¶"""
    try:
        print(f"ğŸ”— å¼€å§‹åˆå¹¶æ ‡å‡†åŒ–è§†é¢‘...", flush=True)
        
        output_folder = f"/app/videos/merged/{account_name}"
        os.makedirs(output_folder, exist_ok=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆç®€åŒ–ç‰ˆï¼‰
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"merged_{timestamp}.mp4"
        output_path = os.path.join(output_folder, output_filename)
        
        # åˆ›å»ºconcatæ–‡ä»¶åˆ—è¡¨
        concat_file = f"/tmp/concat_{account_name}_{timestamp}.txt"
        
        with open(concat_file, 'w', encoding='utf-8') as f:
            for video_file in video_files:
                # è½¬ä¹‰å•å¼•å·
                escaped_path = video_file.replace("'", "'\\''")
                f.write(f"file '{escaped_path}'\n")
        
        print(f"ğŸ“ åˆ›å»ºäº†concatæ–‡ä»¶: {concat_file}", flush=True)
        print(f"   åŒ…å« {len(video_files)} ä¸ªè§†é¢‘", flush=True)
        
        # FFmpegåˆå¹¶å‘½ä»¤
        merge_cmd = [
            'ffmpeg',
            '-f', 'concat',
            '-safe', '0',
            '-i', concat_file,
            '-c', 'copy',
            '-y',
            output_path
        ]
        
        print(f"ğŸ¬ æ‰§è¡Œåˆå¹¶å‘½ä»¤...", flush=True)
        result = subprocess.run(
            merge_cmd,
            capture_output=True,
            text=True,
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode != 0:
            raise Exception(f"FFmpegåˆå¹¶å¤±è´¥: {result.stderr}")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        if not os.path.exists(output_path):
            raise Exception(f"åˆå¹¶åçš„æ–‡ä»¶ä¸å­˜åœ¨: {output_path}")
        
        file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… åˆå¹¶æˆåŠŸ! è¾“å‡ºæ–‡ä»¶: {output_path} ({file_size_mb:.1f}MB)", flush=True)
        
        # æ¸…ç†concatæ–‡ä»¶
        try:
            os.remove(concat_file)
        except:
            pass
        
        return {
            "merged": 1,
            "skipped": 0,
            "failed": 0,
            "output_file": output_path,
            "file_size_mb": round(file_size_mb, 2),
            "video_count": len(video_files)
        }
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}", flush=True)
        import traceback
        traceback.print_exc()
        return {
            "merged": 0,
            "skipped": 0,
            "failed": 1,
            "error": str(e)
        }

# å¯åŠ¨åå°å·¥ä½œè¿›ç¨‹ï¼ˆåœ¨æ¨¡å—åŠ è½½æ—¶ç«‹å³å¯åŠ¨ï¼‰
worker_thread = threading.Thread(target=process_merge_queue, daemon=True)
worker_thread.start()
print("ğŸ”„ åå°åˆå¹¶é˜Ÿåˆ—ç›‘å¬å™¨å·²å¯åŠ¨", flush=True)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000)
